Cosine similarity is a measure to quantify the similarity of two texts. Every document is represented as a vector in vector space. For the vector representation of these documents in three-dimensional space, each word in the document has an orientation.  The vector orientation of the entire document is the combined vector orientation of each term in the document. Cosine similarity considers the proximity of the two documents based on the angle between the combined vector representation of those documents. The vector representation of the document depends upon the presence or absence of every unique word in the entire corpus. Two documents that are the same will have the same vector representation and the angle between them will be zero resulting in cosine similarity as 1 whereas documents that do not have any term in common will have cosine similarity as 0. Other documents will have a score between 0 and 1 depending upon their degree of similarity. 

Cosine similarity is a metric that considers only the orientation of the vectors and not their magnitude. Hence document size does not influence the computation of cosine similarity. Since we are dealing with specific vocabulary in our study, our hypothesis was that cosine similarity is a fair measure to assess the presence of specific information. We, therefore, created a fake document with our vocabulary of LARC terms in order to compute its cosine similarity with every other document in the corpus. The cosine similarity score calculated for each document in the corpus could act as an indicator of how strongly correlated a document is with the desired vocabulary. The value of cosine similarity isbetween 0 and 1 for each document depending upon how many terms from the vocabulary did the document contain.

We wanted to use cosine similarity scores as an additional feature in our Logistic Regression classification model along with bigrams. We have used a method known as ‘Feature Union’ to combine multiple features to train a model using the Scikit learn library in Python. In this framework, we have created a ‘Pipeline’ that can combine the two features i.e. ‘Bigrams’ and ‘Cosine Similarity’. Thus, both features go through the pipeline to be used in the classification. Feature Union basically concatenates different features together to form a new set of features. This framework helped us use cosine similarity as an additional feature in the Logistic Regression classification model.
