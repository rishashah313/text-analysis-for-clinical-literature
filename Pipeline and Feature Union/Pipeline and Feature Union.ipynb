{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a framework for concatenation of multiple features (feature union) in a pipeline for logistic regression. The data we use is webpages from health organization websites that need to be classified as related to contraception or not. We feed the pipeline with a cosine similarity score of every document computed with relevant clinical vocabulary in an attempt to train a model better to identify the class of a document.\n",
    "\n",
    "Cosine similarity is a metric that considers only the orientation of the vectors and not their magnitude. Hence document size does not influence the computation of cosine similarity. Since we are dealing with specific vocabulary in our study, our hypothesis was that cosine similarity is a fair measure to assess the presence of specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cosine</th>\n",
       "      <th>Cosine_expandedVocab</th>\n",
       "      <th>Cosine_expandedVocab_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Other Class</td>\n",
       "      <td>b hyperthyroid caus symptom test diagnosi trea...</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>0.004109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Other Class</td>\n",
       "      <td>b health medic new and doctor s view A Z index...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Other Class</td>\n",
       "      <td>b leg pain symptom sign caus treatment ndoctor...</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.002075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Other Class</td>\n",
       "      <td>b what Is compart syndrom surgeri symptom trea...</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.002024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Other Class</td>\n",
       "      <td>b diseas condit A Z list P on medicinenet com ...</td>\n",
       "      <td>0.023041</td>\n",
       "      <td>0.021658</td>\n",
       "      <td>0.026727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     Category                                               Text  \\\n",
       "0           0  Other Class  b hyperthyroid caus symptom test diagnosi trea...   \n",
       "1           1  Other Class  b health medic new and doctor s view A Z index...   \n",
       "2           2  Other Class  b leg pain symptom sign caus treatment ndoctor...   \n",
       "3           3  Other Class  b what Is compart syndrom surgeri symptom trea...   \n",
       "4           4  Other Class  b diseas condit A Z list P on medicinenet com ...   \n",
       "\n",
       "     Cosine  Cosine_expandedVocab  Cosine_expandedVocab_stemmed  \n",
       "0  0.001644              0.003462                      0.004109  \n",
       "1  0.000000              0.000000                      0.000000  \n",
       "2  0.002355              0.002267                      0.002075  \n",
       "3  0.001552              0.002179                      0.002024  \n",
       "4  0.023041              0.021658                      0.026727  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"stemmed_data_df.csv\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = df\n",
    "training_labels = training_data['Category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "\n",
    "\n",
    "text = Pipeline([\n",
    "    ('selector', TextSelector(key='Text')),\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(2, 2), use_idf=True, stop_words = 'english', max_df=0.95,  min_df=0.05))\n",
    "])\n",
    "\n",
    "\n",
    "text.fit_transform(training_data)\n",
    "\n",
    "cosine_expandedVocab_stemmed = Pipeline([\n",
    "    ('selector', NumberSelector(key='Cosine_expandedVocab_stemmed'))\n",
    "\n",
    "])\n",
    "\n",
    "cosine_expandedVocab_stemmed.fit_transform(training_data)\n",
    "\n",
    "\n",
    "def get_stemmed_data(data):\n",
    "    corpus = []\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        text = re.sub('[^a-zA-Z]', ' ', data['Text'].values[i])  # for 1st review i = 0\n",
    "        text = text.split()\n",
    "        #print(text)\n",
    "        ps = PorterStemmer()\n",
    "        text = [ps.stem(word) for word in text]\n",
    "        text = ' '.join(text)\n",
    "        #print(text)\n",
    "        corpus.append(text)\n",
    "        #print(corpus)\n",
    "    return corpus\n",
    "\n",
    "def store_goldStd_against_pred(goldStd_labels, pred_labels):\n",
    "    pred_goldStd = [[[goldStd_labels[i]], pred] for i, pred in enumerate(pred_labels)]\n",
    "    return pred_goldStd\n",
    "\n",
    "\n",
    "def compute_num_correct_preds(pred_goldStd_list):\n",
    "    correct = 0\n",
    "    \n",
    "    for item in pred_goldStd_list:\n",
    "        \n",
    "        goldStd = item[0]\n",
    "        pred = item[1]\n",
    "\n",
    "        for label in goldStd:\n",
    "            if label in pred:\n",
    "                correct += 1\n",
    "               \n",
    "    \n",
    "    return correct\n",
    "\n",
    "def compute_metrics( LARC_class_goldStd_labels, other_class_goldStd_labels, LARC_class_predictions, other_class_predictions):\n",
    "    TPpred = store_goldStd_against_pred(LARC_class_goldStd_labels, LARC_class_predictions)\n",
    "    FNpred = store_goldStd_against_pred(other_class_goldStd_labels, LARC_class_predictions)\n",
    "    TNpred = store_goldStd_against_pred(other_class_goldStd_labels, other_class_predictions)\n",
    "    FPpred = store_goldStd_against_pred(LARC_class_goldStd_labels, other_class_predictions)\n",
    "\n",
    "    TP = compute_num_correct_preds(TPpred)\n",
    "    FN = compute_num_correct_preds(FNpred)\n",
    "    TN = compute_num_correct_preds(TNpred)\n",
    "    FP = compute_num_correct_preds(FPpred)\n",
    "\n",
    "    Metric = []\n",
    "    Score = []\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    Metric.append(\"Precision\")\n",
    "    Score.append(precision)\n",
    "    \n",
    "    recall = TP / (TP + FN)\n",
    "    Metric.append(\"Recall\")\n",
    "    Score.append(recall)\n",
    "    \n",
    "    accuracy = (TP + TN) / (len(LARC_class_goldStd_labels) + len(other_class_goldStd_labels))\n",
    "    Metric.append(\"accuracy\")\n",
    "    Score.append(accuracy)\n",
    "    \n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    Metric.append(\"f1\")\n",
    "    Score.append(f1)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    data = [['TP', TP], ['FP', FP], ['TN', TN], ['FN', FN], ['precision', precision], ['recall', recall],\n",
    "                ['accuracy', accuracy], ['f1', f1]]\n",
    "\n",
    "    df_metricScores = pd.DataFrame(data, columns=['Metrics', 'Score'])\n",
    "    accuracy_f1 = [accuracy, f1]\n",
    "    return df_metricScores, accuracy_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "other_class = pd.read_csv(\"other class test file.csv\")\n",
    "other_class_df = pd.DataFrame(index=range(len(other_class)))\n",
    "\n",
    "other_class_stemmed = get_stemmed_data(other_class)\n",
    "other_class_stemmed_df = pd.DataFrame(other_class_stemmed, columns = ['Text'])\n",
    "\n",
    "arr = []\n",
    "arr2 = []\n",
    "cosine = []\n",
    "\n",
    "#create a tuple for predictions using a Cosine Similarity score that is already created and the text from the page \n",
    "for i in range(len(other_class)):\n",
    "    arr = []\n",
    "    arr.append(other_class_stemmed_df['Text'].iloc[i])\n",
    "    arr.append(other_class['Cosine_expandedVocab_stemmed'].iloc[i])\n",
    "    arr2.append(arr)\n",
    "\n",
    "dfObj_of_otherClass = pd.DataFrame(arr2, columns=['Text', 'Cosine_expandedVocab_stemmed'])\n",
    "\n",
    "\n",
    "LARC_class = pd.read_csv(\"larc test file.csv\")\n",
    "LARC_class_df = pd.DataFrame(index=range(len(LARC_class)))\n",
    "\n",
    "LARC_class_stemmed = get_stemmed_data(LARC_class)\n",
    "LARC_class_stemmed_df = pd.DataFrame(LARC_class_stemmed, columns = ['Text'])\n",
    "\n",
    "arr = []\n",
    "arr2 = []\n",
    "cosine = []\n",
    "\n",
    "for i in range(len(LARC_class)):\n",
    "    arr = []\n",
    "    arr.append(LARC_class_stemmed_df['Text'].iloc[i])\n",
    "    arr.append(LARC_class['Cosine_expandedVocab_stemmed'].iloc[i])\n",
    "    arr2.append(arr)\n",
    "\n",
    "dfObj_of_LARC_class = pd.DataFrame(arr2, columns=['Text', 'Cosine_expandedVocab_stemmed'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "#choose the features, here we choose only text features to train the model\n",
    "\n",
    "feats = FeatureUnion([('text', text)\n",
    "                      ])\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(training_data)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('classifier', LogisticRegression(verbose=1, solver='liblinear', random_state=0, C=5, penalty='l2', max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(training_data, training_labels)\n",
    "\n",
    "other_class_predictions = pipeline.predict(dfObj_of_otherClass)\n",
    "LARC_class_predictions = pipeline.predict(dfObj_of_LARC_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>115.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FP</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.761589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.837748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.824373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Metrics       Score\n",
       "0         TP  115.000000\n",
       "1         FP   13.000000\n",
       "2         TN  138.000000\n",
       "3         FN   36.000000\n",
       "4  precision    0.898438\n",
       "5     recall    0.761589\n",
       "6   accuracy    0.837748\n",
       "7         f1    0.824373"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_class_goldStd_labels = other_class['Class']\n",
    "LARC_class_goldStd_labels = LARC_class['Class']\n",
    "\n",
    "\n",
    "df_metricScores, accuracy_f1 = compute_metrics( LARC_class_goldStd_labels, other_class_goldStd_labels, LARC_class_predictions, other_class_predictions)\n",
    "df_metricScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "#choose the features, here we choose text features as well as the cosine similarity score for every document to train the model\n",
    "\n",
    "\n",
    "feats = FeatureUnion([('text', text),\n",
    "                      ('cosine_expandedVocab_stemmed', cosine_expandedVocab_stemmed)])\n",
    "\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(training_data)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('classifier', LogisticRegression(verbose=1, solver='liblinear', random_state=0, C=5, penalty='l2', max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(training_data, training_labels)\n",
    "\n",
    "other_class_predictions = pipeline.predict(dfObj_of_otherClass)\n",
    "LARC_class_predictions = pipeline.predict(dfObj_of_LARC_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FP</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>144.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.938053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.701987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.827815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.803030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Metrics       Score\n",
       "0         TP  106.000000\n",
       "1         FP    7.000000\n",
       "2         TN  144.000000\n",
       "3         FN   45.000000\n",
       "4  precision    0.938053\n",
       "5     recall    0.701987\n",
       "6   accuracy    0.827815\n",
       "7         f1    0.803030"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_class_goldStd_labels = other_class['Class']\n",
    "LARC_class_goldStd_labels = LARC_class['Class']\n",
    "\n",
    "\n",
    "df_metricScores, accuracy_f1 = compute_metrics( LARC_class_goldStd_labels, other_class_goldStd_labels, LARC_class_predictions, other_class_predictions)\n",
    "df_metricScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it seems that adding cosine similarity as a feature does not improve the performance. Further error analysis needs to be done to engineer the features better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
