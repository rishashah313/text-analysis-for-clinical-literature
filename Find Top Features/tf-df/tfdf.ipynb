{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 40821)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import operator\n",
    "\n",
    "directory = 'Training/'# Directory of corpus\n",
    "newcorpus = PlaintextCorpusReader(corpusdir, '.*')\n",
    "\n",
    "numbers = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "documents = []\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    f = open(directory + filename)\n",
    "    lines = f.read()\n",
    "    training_doc_words = word_tokenize(lines)\n",
    "    documents.append(lines)\n",
    "    \n",
    "#create a count vectorizer for the corpus\n",
    "cv=CountVectorizer(ngram_range = (1,2),stop_words = 'english')\n",
    "\n",
    "#fit transform the count vectorizer on the corpus\n",
    "word_count_vector=cv.fit_transform(documents)\n",
    "word_count_vector.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40821\n"
     ]
    }
   ],
   "source": [
    "#create a vector representation of the documents\n",
    "documents = word_count_vector.toarray()\n",
    "\n",
    "#total number of features used in this model\n",
    "feature_names = cv.get_feature_names()\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health\n",
      "control\n",
      "birth\n",
      "iud\n",
      "birth control\n",
      "women\n",
      "use\n",
      "doctor\n",
      "office\n",
      "medical\n",
      "pregnancy\n",
      "sex\n",
      "care\n",
      "disease\n",
      "ca\n",
      "clinic\n",
      "information\n",
      "contraception\n",
      "help\n",
      "topics\n",
      "iuds\n",
      "methods\n",
      "blood\n",
      "conditions\n",
      "emergency\n",
      "pregnant\n",
      "uterus\n",
      "risk\n",
      "does\n",
      "policy\n",
      "pain\n",
      "body\n",
      "problems\n",
      "office location\n",
      "mayo\n",
      "search\n",
      "xbb\n",
      "infections\n",
      "pill\n",
      "need\n",
      "privacy\n",
      "symptoms\n",
      "years\n",
      "pid\n",
      "medicinenet\n",
      "prevent\n",
      "emergency contraception\n",
      "time\n",
      "ste\n",
      "na\n",
      "health care\n",
      "website\n",
      "using\n",
      "common\n",
      "style\n",
      "privacy policy\n",
      "like\n",
      "periods\n",
      "nbirth\n",
      "nbirth control\n",
      "insurance\n",
      "pills\n",
      "https\n",
      "health topics\n",
      "living\n",
      "center\n",
      "signs\n",
      "vagina\n",
      "syndrome\n",
      "include\n",
      "don\n",
      "bv\n",
      "woman\n",
      "treatment\n",
      "period\n",
      "work\n",
      "www\n",
      "sexually\n",
      "nhow\n",
      "xae\n",
      "office locations\n",
      "skin\n",
      "nif\n",
      "width\n",
      "style display\n",
      "paragard\n",
      "partner\n",
      "women health\n",
      "inserted\n",
      "pressure\n",
      "want\n",
      "owh helpline\n",
      "insertion\n",
      "vaginal\n",
      "transmitted infections\n",
      "intrauterine device\n",
      "sexually transmitted\n",
      "terms\n",
      "nutrition\n",
      "nyour\n",
      "sterilization\n",
      "research\n",
      "iframe\n",
      "webmd\n",
      "mental\n",
      "services\n",
      "hormone\n",
      "stroke\n",
      "talk\n",
      "rd\n",
      "female\n",
      "online\n",
      "usually\n",
      "sperm\n",
      "ok\n",
      "stds\n",
      "terms use\n",
      "weight\n",
      "resources\n",
      "think\n",
      "sure\n",
      "supplements\n",
      "width style\n",
      "physicians\n",
      "uterine\n",
      "sexual health\n",
      "sugar\n",
      "way\n",
      "ring\n",
      "tissue\n",
      "year\n",
      "works\n",
      "vaginosis\n",
      "tubes\n",
      "womb\n",
      "week\n",
      "tubal\n",
      "states\n",
      "www facebook\n",
      "sheet\n",
      "zip\n",
      "yeast\n",
      "xbb nsign\n",
      "study\n",
      "treat\n",
      "weeks\n",
      "vs\n",
      "vasectomy\n",
      "zip city\n",
      "string\n",
      "severe\n",
      "xaas\n",
      "yeast infections\n",
      "women pregnant\n",
      "vein\n",
      "ways\n",
      "topic\n",
      "warning signs\n",
      "webmd provider\n",
      "uterus womb\n",
      "womenshealth\n",
      "xba\n",
      "work npopular\n",
      "working\n",
      "yes\n",
      "won\n",
      "women use\n",
      "village ca\n",
      "worse\n",
      "xbb nexplore\n",
      "young\n",
      "xbb nfrisky\n",
      "zoloft health\n",
      "younger\n",
      "zip code\n",
      "yes help\n",
      "ztests procedures\n",
      "yesnoare\n",
      "yourfriend address\n",
      "young adults\n",
      "zolna\n",
      "zz arecently\n"
     ]
    }
   ],
   "source": [
    "#Compute document frequency\n",
    "dfs = []\n",
    "for i, word in enumerate(feature_names):\n",
    "    df = 0\n",
    "    for document in documents:\n",
    "        df = df + document[i]\n",
    "    dfs.append(df)\n",
    "\n",
    "#Compute the term frequency\n",
    "count_list = word_count_vector.toarray().sum(axis=0)\n",
    "\n",
    "#Compute tfdfs\n",
    "tfdfs = dfs * count_list\n",
    "\n",
    "#Create dictionary of term against tfdfs\n",
    "featureTfdf_Dict = {}\n",
    "for i,word in enumerate(feature_names):\n",
    "    featureTfdf_Dict[word] = tfdfs[i]\n",
    "    \n",
    "#omit features containing numerical values\n",
    "num_Flag = False\n",
    "featureTfdf_Dict_num_omit = {}\n",
    "featureTfdf_Dict_num_omitted = []\n",
    "for k, v in featureTfdf_Dict.items():\n",
    "    for n in numbers:\n",
    "        if n in k:\n",
    "            num_Flag = True\n",
    "            break\n",
    "    if num_Flag == False:\n",
    "        featureTfdf_Dict_num_omitted.append(k)\n",
    "    num_Flag = False\n",
    "    \n",
    "\n",
    "featureTfdf_Dict_new = {}\n",
    "for term in featureTfdf_Dict_num_omitted:\n",
    "    featureTfdf_Dict_new[term] = featureTfdf_Dict[term]\n",
    "    \n",
    "#Write top features to file sorted based on their tfdf values\n",
    "sorted_d = dict( sorted(((value, key) for (key,value) in featureTfdf_Dict_new.items()), key=operator.itemgetter(0),reverse=True))\n",
    "file = open(\"tfdf features with weights.txt\",\"w\")\n",
    "file1 = open(\"tfdf features only terms.txt\",\"w\")\n",
    "for k, v in sorted_d.items():\n",
    "    file.write('{0} {1}\\n'.format(k, v))\n",
    "    file1.write('{0} \\n'.format(v))\n",
    "    print(v)\n",
    "    \n",
    "file.close()\n",
    "file1.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
